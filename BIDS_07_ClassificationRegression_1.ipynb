{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aim to complete as much of this tutorial on your own *before* coming to the practical session.\n",
    "\n",
    "Use the practical session to get help for any aspect you do not understand or were unable to complete."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification and Regression 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning objectives\n",
    "1. Implement PLS, Ridge, and Lasso and Elastic Net regression using the popular python library [sklearn](https://scikit-learn.org/stable/)\n",
    "2. Visualise the regression results \n",
    "3. Explore different metrics to evaluate the model performance in regression settings\n",
    "4. Investigate the effect of scaling the data on the model performance\n",
    "5. Apply penalised (logistic) regression for classification "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import specific packages and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTS\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn import metrics\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, scale\n",
    "from sklearn.model_selection import train_test_split, KFold, RepeatedKFold, GridSearchCV\n",
    "from sklearn import model_selection\n",
    "from sklearn.linear_model import LogisticRegression, Lasso, Ridge, ElasticNet # for Lasso and Elastic Net logistic regression\n",
    "from sklearn.cross_decomposition import PLSRegression # for PLS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial we will use some plasma metabolomics data to predict BMI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../Data-main/diabetes_metabolomics_plasma.csv') #import the data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head() ### "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see this data set in its current format is not suitable for our algorithms. \n",
    "In python column indices start from 0, we want to subset only the metabolomics columns for feature scaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create feature matrix and target vector\n",
    "X = df.iloc[:,6:]\n",
    "y = df['BMI']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head() # Try using the .tail() function -- it is similar to the .head() function but displays the last 5 rows "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to test our alogrithms we need to set aside some of the data we have. This is practice for machine learning models. We will use 80% of our data to train our model, and the remaining 20% will be used to test the performance of our model. \n",
    "\n",
    "Scikit-Learn has a function ```train_test_split``` to easily do this for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enter your CID here, or date of birth, or another number of your choosing to use as random state\n",
    "CID = 0\n",
    "\n",
    "# remember to check the documentation of each algorithm if setting the random_state is needed\n",
    "# for this tutorial and all upcoming tutorials..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the df into 80% train 20% test data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=CID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is crucial that all of the data it is comparing is on the same scale. In our metabolomics data, most of the data is continuous. We will scale the data using the ```StandardScaler()``` shown in the previous tutorials. \n",
    "\n",
    "When scaling your data you want to fit the model to your training data, and only transform your testing data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize Data\n",
    "\n",
    "# Instantiate scaler model\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit and Transform X_train\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "# Transform X_test\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PLS Regression \n",
    "\n",
    "We will first look at [PLSRegression()](https://scikit-learn.org/stable/modules/generated/sklearn.cross_decomposition.PLSRegression.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plsr = PLSRegression(n_components=1)\n",
    "plsr.fit(X_train_scaled, y_train)\n",
    "y_pred = plsr.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Metrics\n",
    "Today we will use the following metrics\n",
    "- explained_variance_score\n",
    "- r2_score\n",
    "- mean_squared_error\n",
    "- mean_absolute_error\n",
    "- SMAPE (Symmetric Mean Absolute Percentage Error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is just a helper function to get the smape metric - no need to change \n",
    "def smape(A, F):\n",
    "    out =  100/len(A) * np.sum(2 * np.abs(F - A) / (np.abs(A) + np.abs(F)))\n",
    "    return(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Here we will define several helper functions to compute these metrics \n",
    "def RegmodelPerformance(y_true, y_pred):\n",
    "    exp_var = metrics.explained_variance_score(y_true, y_pred)\n",
    "    r_square = metrics.r2_score(y_true, y_pred)\n",
    "    MSE = metrics.mean_squared_error(y_true, y_pred)\n",
    "    MAE = metrics.mean_absolute_error(y_true, y_pred)\n",
    "    SMAPE = smape(np.array(y_true), y_pred)\n",
    "    return(exp_var, r_square, MSE, MAE, SMAPE)\n",
    "\n",
    "def printPerformance(y_true, y_pred):\n",
    "    exp_var, r_square, MSE, MAE, SMAPE = RegmodelPerformance(y_true, y_pred)\n",
    "    print(\"explained variance score = \" \"%.4f\" % exp_var)\n",
    "    print(\"R2 = \" \"%.4f\" % r_square)\n",
    "    print(\"MSE = \" \"%.4f\" % MSE)\n",
    "    print(\"MAE = \" \"%.4f\" % MAE)\n",
    "    print(\"SMAPE = \" \"%.4f\" % SMAPE)\n",
    "    np.set_printoptions(precision=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "printPerformance(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whilst we have trained and fit a PLS model, we have not optimised the number of components. We also have not done any cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = RepeatedKFold(n_splits=10, n_repeats=1, random_state=42)\n",
    "\n",
    "mse = []\n",
    "n = len(X_train)\n",
    "\n",
    "# Calculate MSE using cross-validation, adding one component at a time\n",
    "for i in np.arange(1, 10):\n",
    "    pls = PLSRegression(n_components=i)\n",
    "    score = -1*model_selection.cross_val_score(pls, scale(X_train), y_train, cv=cv,\n",
    "               scoring='neg_mean_squared_error').mean()\n",
    "    mse.append(score)\n",
    "\n",
    "#plot test MSE vs. number of components - lower is better (smaller error)\n",
    "plt.plot(np.arange(1, 10),mse)\n",
    "plt.xlabel('Number of PLS Components')\n",
    "plt.ylabel('MSE')\n",
    "plt.title('Cross-validation Metrics')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, we can also run our own 10-fold CV."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try below to fill in with your own code to the cross-validation loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_vars = []\n",
    "r_squares = []\n",
    "MSEs = []\n",
    "MAEs = []\n",
    "SMAPEs = [] \n",
    "from sklearn.model_selection import KFold\n",
    "kf = KFold(n_splits=2, random_state=CID, shuffle=False)\n",
    "kf.get_n_splits(X_train_scaled)\n",
    "for train_index, test_index in kf.split(X_train):\n",
    "    # add here code to estimate the optimal number of components from the training set\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate MSE using cross-validation, \n",
    "numSplits = 10\n",
    "exp_var_mean = 0 \n",
    "r_square_mean = 0\n",
    "MSE_mean = 0 \n",
    "MAE_mean = 0\n",
    "SMAPE_mean = 0 \n",
    "kf = KFold(n_splits=numSplits, random_state=CID, shuffle=False)\n",
    "kf.get_n_splits(X_train)\n",
    "\n",
    "for train_index, val_index in kf.split(X_train):\n",
    "    X_traincv = X_train.iloc[train_index]\n",
    "    X_valcv = X_train.iloc[val_index]\n",
    "    y_traincv = y_train.iloc[train_index]\n",
    "    y_valcv = y_train.iloc[val_index]\n",
    "    X_traincv = scaler.fit_transform(X_traincv)\n",
    "    X_valcv = scaler.transform(X_valcv)\n",
    "    pls = PLSRegression(n_components=2) ## i is the no. components, consider \n",
    "    pls.fit(X_traincv, y_traincv)\n",
    "    ypred = pls.predict(X_valcv)\n",
    "    exp_var, r_square, MSE, MAE, SMAPE = RegmodelPerformance(y_valcv, ypred)\n",
    "    exp_var_mean+= exp_var\n",
    "    r_square_mean += r_square\n",
    "    MSE_mean += MSE\n",
    "    MAE_mean += MAE\n",
    "    SMAPE_mean += SMAPE\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## in the space below compute the mean and standard deviations of your metrics\n",
    "## I have done the first one as an example\n",
    "\n",
    "print(exp_var_mean/numSplits)\n",
    "print(r_square_mean/numSplits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now have a think how you might do this and optimise the number of components at the same time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_vars = []\n",
    "r_squares = []\n",
    "MSEs = []\n",
    "MAEs = []\n",
    "SMAPEs = [] \n",
    "# Calculate MSE using cross-validation, adding one component at a time\n",
    "for i in range(1, 12):   #Here we are defining how many components \n",
    "    exp_var_mean = 0 \n",
    "    r_square_mean = 0\n",
    "    MSE_mean = 0 \n",
    "    MAE_mean = 0\n",
    "    SMAPE_mean = 0 \n",
    "    kf = KFold(n_splits=10, random_state=None, shuffle=False) ## can also input and value you like for random state but also set shuffle = Ture\n",
    "    kf.get_n_splits(X_train)\n",
    "\n",
    "    for train_index, val_index in kf.split(X_train):\n",
    "        X_traincv = X_train.iloc[train_index]\n",
    "        X_valcv = X_train.iloc[val_index]\n",
    "        y_traincv = y_train.iloc[train_index]\n",
    "        y_valcv = y_train.iloc[val_index]\n",
    "        X_traincv = scaler.fit_transform(X_traincv)\n",
    "        X_valcv = scaler.fit_transform(X_valcv)\n",
    "        pls = PLSRegression(n_components=i) ## i is the no. components\n",
    "        pls.fit(X_traincv, y_traincv)\n",
    "        ypred = pls.predict(X_valcv)\n",
    "        exp_var, r_square, MSE, MAE, SMAPE = RegmodelPerformance(y_valcv, ypred)\n",
    "        exp_var_mean+= exp_var\n",
    "        r_square_mean += r_square\n",
    "        MSE_mean += MSE\n",
    "        MAE_mean += MAE\n",
    "        SMAPE_mean += SMAPE\n",
    "    exp_vars.append(exp_var_mean/10)\n",
    "    r_squares.append(r_square_mean/10)\n",
    "    MSEs.append(MSE_mean/10)\n",
    "    MAEs.append(MAE_mean/10)\n",
    "    SMAPEs.append(SMAPE_mean/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(MSEs) ## printing len MSE should be the same as however many components you have looked into "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot test MSE vs. number of components\n",
    "plt.plot(range(1,12),MSEs)\n",
    "plt.xlabel('Number of PLS Components')\n",
    "plt.ylabel('MSE')\n",
    "plt.title('Cross-validation Metrics')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results vary slightly depending on the split seed values, although it looks like the best model is the one with 4 components? Do you get the same result? Do you want to use a different metric than MSE to find the optimum?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_comp_opt = np.array(MSEs).argsort()[0]+1 # finding the index of the lowest value\n",
    "plsbest = PLSRegression(n_components=n_comp_opt)\n",
    "plsbest.fit(X_train_scaled, y_train)\n",
    "y_pred = plsbest.predict(X_test_scaled)\n",
    "printPerformance(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature importance\n",
    "\n",
    "What are the most important features in the model? Inspect the ```coef_``` attribute and see which variables have the biggest (absolute) weights (loadings) in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# get feature coefficients\n",
    "coef = plsbest.coef_\n",
    "# get feature names\n",
    "cnames = list(X.columns)\n",
    "# sort these\n",
    "_, coef, cnames = zip(*sorted(zip(abs(coef), coef, cnames),reverse=True))\n",
    "coef = np.concatenate(coef)\n",
    "# show top 10 features\n",
    "nfeat = 10\n",
    "ind = [*range(nfeat, 0, -1)]\n",
    "cp = coef[0:nfeat]\n",
    "ind = np.asarray(ind)\n",
    "# plotting the results\n",
    "fig, ax = plt.subplots()\n",
    "ax.barh(ind,cp)\n",
    "ax.set_yticks(ind, labels=cnames[0:nfeat])\n",
    "ax.set_xlabel('PLS loading weights')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lasso \n",
    "\n",
    "Now we will use the [Lasso()](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html) function. We will optimise the model parameters using a [GridSearch](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model\n",
    "model = Lasso()\n",
    "# define model evaluation method\n",
    "cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=CID)\n",
    "# define grid\n",
    "grid = dict()\n",
    "grid['alpha'] = np.arange(0, 1, 0.01)\n",
    "# define search\n",
    "search = GridSearchCV(model, grid, scoring='neg_mean_absolute_error', cv=cv, n_jobs=-1)\n",
    "# perform the search\n",
    "results = search.fit(X_train_scaled, y_train)\n",
    "# summarize\n",
    "print('MAE: %.3f' % (results.best_score_*-1))\n",
    "print('Config: %s' % results.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict the test set now, how does this compare with PLS?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = search.predict(X_test_scaled)\n",
    "printPerformance(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature importance\n",
    "\n",
    "What are the most important features in the model? Inspect the ```coef_``` attribute and see which variables have non-zero weights in the model. Note that the GridSearchCV model does not have this attribute, so we must get it another way..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the best parameters\n",
    "best_param = results.best_params_\n",
    "\n",
    "# fit a new model with these parameters\n",
    "lasso = Lasso(alpha=results.best_params_[\"alpha\"])\n",
    "lasso.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get feature coefficients\n",
    "coef = lasso.coef_\n",
    "# get feature names\n",
    "cnames = list(X.columns)\n",
    "# sort these\n",
    "acoef, coef, cnames = zip(*sorted(zip(abs(coef), coef, cnames),reverse=True))\n",
    "coef = np.asarray(coef)\n",
    "nonzerofeat = sum(np.asarray(acoef)>0)\n",
    "ind = np.asarray([*range(nonzerofeat, 0, -1)])\n",
    "# plotting the results\n",
    "fig, ax = plt.subplots(figsize=(10,15))\n",
    "ax.barh(ind,coef[0:nonzerofeat])\n",
    "ax.set_yticks(ind, labels=cnames[0:nonzerofeat])\n",
    "ax.set_xlabel('Lasso coefficients')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try right the same as the above but now using the [Ridge()](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html) function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model and try the rest yourself\n",
    "model = Ridge()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model\n",
    "model = Ridge()\n",
    "# define model evaluation method\n",
    "cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=CID)\n",
    "# define grid\n",
    "grid = dict()\n",
    "grid['alpha'] = np.arange(0, 1, 0.01)\n",
    "# define search\n",
    "search = GridSearchCV(model, grid, scoring='neg_mean_absolute_error', cv=cv, n_jobs=-1)\n",
    "# perform the search\n",
    "results = search.fit(X_train_scaled, y_train)\n",
    "# summarize\n",
    "print('MAE: %.3f' % (results.best_score_*-1))\n",
    "print('Config: %s' % results.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test set performane \n",
    "y_pred = search.predict(X_test_scaled)\n",
    "printPerformance(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature importance\n",
    "\n",
    "What are the most important features in the model? Inspect the ```coef_``` attribute similarly to Lasso by creating a new model, and plotting the top n features as with PLS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the best parameters\n",
    "best_param = results.best_params_\n",
    "\n",
    "# fit a new model with these parameters\n",
    "ridge = Ridge(alpha=results.best_params_[\"alpha\"])\n",
    "ridge.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get feature coefficients\n",
    "coef = ridge.coef_\n",
    "# get feature names\n",
    "cnames = list(X.columns)\n",
    "# sort these\n",
    "_, coef, cnames = zip(*sorted(zip(abs(coef), coef, cnames),reverse=True))\n",
    "coef = np.asarray(coef)\n",
    "# show top 10 features\n",
    "nfeat = 10\n",
    "ind = [*range(nfeat, 0, -1)]\n",
    "cp = coef[0:nfeat]\n",
    "ind = np.asarray(ind)\n",
    "# plotting the results\n",
    "fig, ax = plt.subplots()\n",
    "ax.barh(ind,cp)\n",
    "ax.set_yticks(ind, labels=cnames[0:nfeat])\n",
    "ax.set_xlabel('Ridge regression coefficients')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare the Ridge results with Lasso and PLS, which model is most predictive, which are the important features?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Elastic-Net "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Elastic Net is a mix of both ridge and lasso, where we can tune both of the parameters.\n",
    "\n",
    "Don't worry if the code below (using the [ElasticNet()](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.ElasticNet.html) function) returns some warnings around the model not converging. However, __do think about what this means...__ (could you change some parameters?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model\n",
    "model = ElasticNet()\n",
    "# define model evaluation method\n",
    "cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=CID)\n",
    "# define grid\n",
    "grid = dict()\n",
    "grid = {\"max_iter\": [10, 50, 100],\n",
    "                      \"alpha\": [0.0001, 0.001, 0.01, 0.1, 1, 10, 100],\n",
    "                      \"l1_ratio\": np.arange(0.0, 1.0, 0.1)}\n",
    "# define search\n",
    "search = GridSearchCV(model, grid, scoring='neg_mean_absolute_error', cv=cv, n_jobs=-1)\n",
    "# perform the search\n",
    "results = search.fit(X_train_scaled, y_train)\n",
    "# summarize\n",
    "print('MAE: %.3f' % (results.best_score_*-1))\n",
    "print('Config: %s' % results.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test set performane \n",
    "y_pred = search.predict(X_test_scaled)\n",
    "printPerformance(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature importance\n",
    "\n",
    "What are the most important features in the model? Inspect the ```coef_``` attribute and see which variables have non-zero weights in the model as with the Lasso model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the best parameters\n",
    "best_param = results.best_params_\n",
    "\n",
    "# fit a new model with these parameters\n",
    "enet = ElasticNet(alpha=results.best_params_[\"alpha\"], l1_ratio=results.best_params_[\"l1_ratio\"], max_iter=results.best_params_[\"max_iter\"])\n",
    "enet.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get feature coefficients\n",
    "coef = enet.coef_\n",
    "# get feature names\n",
    "cnames = list(X.columns)\n",
    "# sort these\n",
    "acoef, coef, cnames = zip(*sorted(zip(abs(coef), coef, cnames),reverse=True))\n",
    "coef = np.asarray(coef)\n",
    "nfeat = sum(np.asarray(acoef)>0)\n",
    "if nfeat > 100:\n",
    "    nfeat = 50 # limit number of features shown, can increase/decrease these numbers\n",
    "ind = np.asarray([*range(nfeat, 0, -1)])\n",
    "# plotting the results\n",
    "fig, ax = plt.subplots(figsize=(10,15))\n",
    "ax.barh(ind,coef[0:nfeat])\n",
    "ax.set_yticks(ind, labels=cnames[0:nfeat])\n",
    "ax.set_xlabel('Elastic Net coefficients')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What's Next?\n",
    "We have walked through how to implement PLS, Ridge, Lasso and Elastic Net regression.\n",
    "\n",
    "For further understanding and practice:\n",
    "- Try using a different scaling to see how it affects results: ```robust_scale``` / ```RobustScaler```, ```power_transform``` / ```PowerTransformer```\n",
    "- Change certain paramaters such as number of folds, ncomponents (PLS), alpha and lambda values (Elastic net, Lasso, Ridge), e.g try a larger grid search \n",
    "- Use a different dataset for a regression problem\n",
    "- Implement different error metrics for regression\n",
    "- Implement the classification (logistic regression) versions of these, look at examples here for [Ridge](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.RidgeClassifier.html) and [Lasso and ElasticNet](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html), and you can use a binary vector as input to [PLSRegression](https://scikit-learn.org/stable/modules/generated/sklearn.cross_decomposition.PLSRegression.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your turn\n",
    "Select another dataset from the `Data` folder, import and inspect it.\n",
    "\n",
    "Is there a continuous outcome you can use for regression?\n",
    "\n",
    "If yes, use one of the methods above to model it.\n",
    "\n",
    "If not, try to implement the classification versions of these algorithms.\n",
    "\n",
    "How well do the methods perform on the new dataset? Does the same method as above perform best?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import datasets..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data and perform scaling..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate a regression or classification model..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the important features"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
